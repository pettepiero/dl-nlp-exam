{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 15:22:46.826719: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/programs/cuda/11.8/lib64:/opt/programs/openMPI/4.1.6/lib:/opt/programs/hwloc/2.12.0/lib:/u/dssc/ppette00/miniconda3/lib::\n",
      "2025-05-27 15:22:46.826751: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.models.bert.modeling_tf_bert.TFBertModel at 0x7fb2e3e665c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(['Hello world', 'Hi how are you'], padding=True, truncation=True, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
       "array([[ 101, 7592, 2088,  102,    0,    0],\n",
       "       [ 101, 7632, 2129, 2024, 2017,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor: shape=(2, 6, 768), dtype=float32, numpy=\n",
       "array([[[-0.16888423,  0.13606375, -0.1394004 , ..., -0.62511253,\n",
       "          0.0521724 ,  0.3671451 ],\n",
       "        [-0.3632746 ,  0.14122003,  0.8799881 , ...,  0.10433009,\n",
       "          0.28875768,  0.3726794 ],\n",
       "        [-0.6985945 , -0.6987971 ,  0.06450248, ..., -0.2210367 ,\n",
       "          0.0098687 , -0.59397894],\n",
       "        [ 0.8309827 ,  0.12366682, -0.15119113, ...,  0.10309644,\n",
       "         -0.6779267 , -0.26285204],\n",
       "        [-0.40266672, -0.01928207,  0.5732504 , ..., -0.2065682 ,\n",
       "          0.02338546,  0.2012631 ],\n",
       "        [-0.6228408 , -0.27453476,  0.18117644, ..., -0.12944858,\n",
       "         -0.038391  , -0.05733228]],\n",
       "\n",
       "       [[ 0.0928655 , -0.02636368, -0.12239344, ..., -0.2106351 ,\n",
       "          0.1738637 ,  0.17250948],\n",
       "        [ 0.4074199 , -0.05930912,  0.55234754, ..., -0.67905647,\n",
       "          0.65557426, -0.29456505],\n",
       "        [-0.21155237, -0.6858646 , -0.46280938, ...,  0.15278468,\n",
       "          0.5977423 , -0.9102003 ],\n",
       "        [ 0.39921287, -1.3207805 , -0.08008869, ..., -0.32125443,\n",
       "          0.2557293 , -0.57804495],\n",
       "        [-0.07565212, -1.3393835 ,  0.1816293 , ...,  0.07461083,\n",
       "          0.40318495, -0.7079991 ],\n",
       "        [ 0.59889317, -0.28409368, -0.34899232, ...,  0.304201  ,\n",
       "         -0.436755  , -0.20969653]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(2, 768), dtype=float32, numpy=\n",
       "array([[-0.9061534 , -0.31115308, -0.6216537 , ..., -0.30575225,\n",
       "        -0.64009386,  0.91661745],\n",
       "       [-0.9309669 , -0.3380702 , -0.6216157 , ..., -0.44018954,\n",
       "        -0.6812885 ,  0.93488973]], dtype=float32)>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c76b645c5a42aca46c788bad4c47b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eeb235f73ee4abaa4e64ffb0c33fbb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c305283e8e44efb73585fade29a063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c063ae0853424b608ed9559fe49f9f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a540ce3e7440e9afc45c112059b5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a656688f9342569c5b1bdbbe4e82b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f42442e92ce4d91a72e4a6b85239b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"stanfordnlp/sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'sentence', 'label'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['sentence'], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8cef3cfd7f495dba855700a8b12c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb431d48a06a48ad816cea38adc2dbd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57eb8d4a1a8f4f7cbccf0147db65510a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences_encoded = dataset.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'sentence', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['idx', 'sentence', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'sentence', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting 'input_ids', 'attention_mask', 'token_type_ids', and 'label'\n",
    "# to the tensorflow format. Now if you access this dataset you will get these\n",
    "# columns in `tf.Tensor` format\n",
    "\n",
    "sentences_encoded.set_format(\n",
    "    'tf', \n",
    "    columns=['input_ids', 'attention_mask', 'token_type_ids', 'label']\n",
    ")\n",
    "\n",
    "# setting BATCH_SIZE to 64.\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def order(inp):\n",
    "    '''\n",
    "    This function will group all the inputs of BERT\n",
    "    into a single dictionary and then output it with\n",
    "    labels.\n",
    "    '''\n",
    "    data = list(inp.values())\n",
    "    return {\n",
    "        'input_ids': data[1],\n",
    "        'attention_mask': data[2],\n",
    "        'token_type_ids': data[3]\n",
    "    }, data[0]\n",
    "\n",
    "# converting train split of `emotions_encoded` to tensorflow format\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(sentences_encoded['train'][:])\n",
    "# set batch_size and shuffle\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).shuffle(1000)\n",
    "# map the `order` function\n",
    "train_dataset = train_dataset.map(order, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# ... doing the same for test set ...\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(sentences_encoded['test'][:])\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.map(order, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(64, 66), dtype=int64, numpy=\n",
      "array([[  101,  2019,  2125, ...,     0,     0,     0],\n",
      "       [  101,  8183,  3367, ...,     0,     0,     0],\n",
      "       [  101,  2097,  2022, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 20996, 14227, ...,     0,     0,     0],\n",
      "       [  101,  1037,  2143, ...,     0,     0,     0],\n",
      "       [  101,  1010,  1036, ...,     0,     0,     0]])>, 'attention_mask': <tf.Tensor: shape=(64, 66), dtype=int64, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])>, 'token_type_ids': <tf.Tensor: shape=(64, 66), dtype=int64, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]])>} \n",
      "\n",
      " tf.Tensor(\n",
      "[1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1 0 1 1 1 1 0\n",
      " 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0], shape=(64,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "inp, out = next(iter(train_dataset)) # a batch from train_dataset\n",
    "print(inp, '\\n\\n', out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTForClassification(tf.keras.Model):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    def call(self, inputs):\n",
    "        x = self.bert(inputs)[1]\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = BERTForClassification(model, num_classes=2)\n",
    "\n",
    "classifier.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "   6/1053 [..............................] - ETA: 6:27:24 - loss: 0.7010 - accuracy: 0.5312"
     ]
    }
   ],
   "source": [
    "history = classifier.fit(\n",
    "    train_dataset,\n",
    "    epochs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
