# From Predictions to Decisions: The importance of Joint Predictive Distributions
*Though accurate marginal predictions are sufficient for some applications, in this paper, we show that for a broad class of downstream tasks, accurate joint predictions are essential. In particular, we show that even in simple combinatorial and sequential decision problems, accurate marginal predictions
are insufficient for effective decisions. Rather, accurate joint predictive distributions are required to achieve good
performance. We also show that, in the multi-armed bandit, a classical sequential decision problem, accurate joint
predictive distributions enable near-optimal performance.*
https://arxiv.org/pdf/2107.09224 
## A coin flipping example: marginal vs joint predictive distributions

Suppose that $(Y_t: t=0,1,...)$ are generated by repeated tosses of a biased coin with unknown probability *p* of heads, with 
$Y_{t+1} = 1$ and $Y_{t+1} = 0$ indicating hads and tails, respectively. 
Consider 2 agents with 2 different beliefs:
1. Agent 1 assumes $p=\frac{2}{3}$ and **models the outcome of each coin toss independent conditioned on p**
2. Agent 2 assumes that 
$$
p = 
\begin{cases} 
1 & \text{with probability } \frac{2}{3}, \\
0 & \text{with probability } \frac{1}{3}.
\end{cases}
$$

Denote the outcomes of the two agents $\hat{Y}^{1}_{t+1}$ and $\hat{Y}^{2}_{t+1}$.

The marginal predictive distributions of the two agents are identical:
$$P(\hat{Y}^{1}_{t+1}=0) = \frac{1}{3} = P(\hat{Y}^{2}_{t+1}=0)$$
but for two different reasons. These can be seen when we take the **joint** predictions over a set of $\tau$ data points:

$$P(\hat{Y}^{1}_{1}, \hat{Y}^{1}_{2}, ... , \hat{Y}^{1}_{\tau} =0) = \left(\frac{1}{3}\right)^\tau$$
while
$$P(\hat{Y}^{2}_{1}, \hat{Y}^{2}_{2}, ... , \hat{2}^{1}_{\tau} =0) = \frac{1}{3}$$
because we are always flipping the same coin, which gives all heads $\left(p=1 \right)$ with probability $\frac{2}{3}$.
Therefore, evaluating only the marginal predictions (the joint over $\tau$ data points is marginalized and the prediction for only one element is returned) as above cannot distinguish between the two agents. One must evaluate joint predictions to make this distinction.

## Combinatorial decision problems
Consider the problem of a customer interacting with a recommendation system that proposes $K=2$ movies from an inveentory of $4$ movies $\{X_1, X_2, X_3, X_4\}$. Each $X_i \in \Reals^d$ describes the features of movie $i$, and $d$ is the feature dimension.

The probability that a user will enjoy movie $i$ is modeled by a logistic model $Y_i \sim logit(\phi^{T}_*)$ where $\phi^{T}_*$ describes the preferences of the user and it is unknown from the recommendation system. Therefore $\phi^{T}_*$ can be seen as a random variable, and, for the purpose of this example, suppose that it is drawn from two possible user types $\{\phi_1, \phi_2\}$. The following figure shows the numerical values of this example. The cells contain the probabilities of selection (implied by the logit functions and correct to two decimal places), **that the user will enjoy at least one of the recommended movies**.

![Table 1 from article](./figures/Screenshot%20from%202024-06-20%2011-48-11.png)

Note:
1. An agent that optimizes the expected probability for each movie individually (marginal distributions are on indivisual $X_i$) will end up recommending the pair $(X_3, X_4)$ to an unknown $\theta \sim Unif(\theta_1, \theta_2)$. With these recommendations, it is still possible that a user might not like either movie. The marginal probability is the bottom line of the table above.
2. An agent that considers the joint predictive distributions (pairs of $(X_i, X_j)$) can see that selecting the pair $(X_1, X_2)$ will give close to 100% certainty that the user will enjoy one of the movies.

In combinatorial decision problems where the outcome depends on the joint predictive distribution, optimization based on the marginal predictive distribution is insufficient to guarantee good decisions.