{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a93f7dec8e1b480ebe5f223f3bb01aa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9fef59ee481449a4bc7b0830c31427e6",
              "IPY_MODEL_356b14e619e0427abdfa700af086362e",
              "IPY_MODEL_a5dff17b296d458eaeb3f3974d5c1005"
            ],
            "layout": "IPY_MODEL_b9d893e8265e46e0b051e4f264f80dd4"
          }
        },
        "9fef59ee481449a4bc7b0830c31427e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fcae41536004881b5724a65e4ca6031",
            "placeholder": "​",
            "style": "IPY_MODEL_0320177245c84174b390632a70d5a351",
            "value": "config.json: 100%"
          }
        },
        "356b14e619e0427abdfa700af086362e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae156a0e05d94a6792f4687b6ce4ca2a",
            "max": 481,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_01a1f0dd89c7401eab2a8d1a4ecf9920",
            "value": 481
          }
        },
        "a5dff17b296d458eaeb3f3974d5c1005": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cc945b0b7a64a0c94c74f287f1f789a",
            "placeholder": "​",
            "style": "IPY_MODEL_e376bd1b0ee2455aa838084e94720c99",
            "value": " 481/481 [00:00&lt;00:00, 60.3kB/s]"
          }
        },
        "b9d893e8265e46e0b051e4f264f80dd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fcae41536004881b5724a65e4ca6031": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0320177245c84174b390632a70d5a351": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae156a0e05d94a6792f4687b6ce4ca2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01a1f0dd89c7401eab2a8d1a4ecf9920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8cc945b0b7a64a0c94c74f287f1f789a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e376bd1b0ee2455aa838084e94720c99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f493a4cebb7a4efda5d4538672f0024b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a47e6c3625d44979a68efeaa3ecb495",
              "IPY_MODEL_f77e52f619db4ef59fe9f231c25c3ce0",
              "IPY_MODEL_6183b6c9d464493b84d622aaa8f12a15"
            ],
            "layout": "IPY_MODEL_6c7bc8bd014d44a6b47f9d2358656d66"
          }
        },
        "0a47e6c3625d44979a68efeaa3ecb495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ea17799985245f0b7dafdc5f615da27",
            "placeholder": "​",
            "style": "IPY_MODEL_ada6e035c9544e8aba3a50374c048663",
            "value": "model.safetensors: 100%"
          }
        },
        "f77e52f619db4ef59fe9f231c25c3ce0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b71f90c1247c44f3a054b05f13419c50",
            "max": 498818054,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_206418e04e8b480c974dcf925043d788",
            "value": 498818054
          }
        },
        "6183b6c9d464493b84d622aaa8f12a15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65218e3199a74b339bedecea52d34a30",
            "placeholder": "​",
            "style": "IPY_MODEL_15ccd38e2027443da07631a3a81bf69d",
            "value": " 499M/499M [00:02&lt;00:00, 239MB/s]"
          }
        },
        "6c7bc8bd014d44a6b47f9d2358656d66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ea17799985245f0b7dafdc5f615da27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ada6e035c9544e8aba3a50374c048663": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b71f90c1247c44f3a054b05f13419c50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "206418e04e8b480c974dcf925043d788": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65218e3199a74b339bedecea52d34a30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15ccd38e2027443da07631a3a81bf69d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd50da6beabf44feb46cad4dd2595535": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a489fb803764f17814cd54c90c6c7d3",
              "IPY_MODEL_7faed7d01a9d49e9a1ca1904106e7189",
              "IPY_MODEL_3245721ecd864b09a7470bf8b41b751c"
            ],
            "layout": "IPY_MODEL_57854a38cb5748dfb532fd446dfabbdc"
          }
        },
        "4a489fb803764f17814cd54c90c6c7d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f51cd4848c03423daa1971fbc1d8c160",
            "placeholder": "​",
            "style": "IPY_MODEL_6c7f7d72f81a4df6883bd5b664313136",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "7faed7d01a9d49e9a1ca1904106e7189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35f72693432f419d815aa111344bfd3c",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_abe2e33a918d408a9b0f988fb6ae5451",
            "value": 25
          }
        },
        "3245721ecd864b09a7470bf8b41b751c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a29de0629865458a8bc55ab12620171c",
            "placeholder": "​",
            "style": "IPY_MODEL_c2bf313a92ef4b389ed2fcfd5c201679",
            "value": " 25.0/25.0 [00:00&lt;00:00, 3.93kB/s]"
          }
        },
        "57854a38cb5748dfb532fd446dfabbdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f51cd4848c03423daa1971fbc1d8c160": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c7f7d72f81a4df6883bd5b664313136": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35f72693432f419d815aa111344bfd3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abe2e33a918d408a9b0f988fb6ae5451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a29de0629865458a8bc55ab12620171c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2bf313a92ef4b389ed2fcfd5c201679": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "186a9ef2e10b41f39c7cee4a9c61d5ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_afd9f15f55744bf0adcf972be8b7cf3e",
              "IPY_MODEL_36c328f206e04100be4fb0041daae4c7",
              "IPY_MODEL_ed0616ce2a5c41ad8f33539a82a254db"
            ],
            "layout": "IPY_MODEL_eddc617c6c3d45809f6a2ff260b888e8"
          }
        },
        "afd9f15f55744bf0adcf972be8b7cf3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_652249ef6b0c4c95852cfb345d3dda30",
            "placeholder": "​",
            "style": "IPY_MODEL_31fd72607b364202a322694f2c056d50",
            "value": "vocab.json: 100%"
          }
        },
        "36c328f206e04100be4fb0041daae4c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d394c6b488ba4ab18a9d8e94999e9559",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28879a5078134661957094a4241a097e",
            "value": 898823
          }
        },
        "ed0616ce2a5c41ad8f33539a82a254db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a77bac0ebbf14c1b9173cefe99346207",
            "placeholder": "​",
            "style": "IPY_MODEL_7c5170db10094ff494f2b4980a02bbef",
            "value": " 899k/899k [00:00&lt;00:00, 6.43MB/s]"
          }
        },
        "eddc617c6c3d45809f6a2ff260b888e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "652249ef6b0c4c95852cfb345d3dda30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31fd72607b364202a322694f2c056d50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d394c6b488ba4ab18a9d8e94999e9559": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28879a5078134661957094a4241a097e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a77bac0ebbf14c1b9173cefe99346207": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c5170db10094ff494f2b4980a02bbef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0aa1ec75b474e0ebb64ca7bb2c3f847": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd76ee3edbd54a8bb06f08ef2f7823f4",
              "IPY_MODEL_6512e14a8af14979989be4f728832bfc",
              "IPY_MODEL_08904a7147fe48f7bce477b64388526e"
            ],
            "layout": "IPY_MODEL_97cd229f16b141108d2d680baa087c63"
          }
        },
        "fd76ee3edbd54a8bb06f08ef2f7823f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c14b19c10b944918b5a24d705a36abf",
            "placeholder": "​",
            "style": "IPY_MODEL_b035516ae4dc49a59fe20ad9c8c22e98",
            "value": "merges.txt: 100%"
          }
        },
        "6512e14a8af14979989be4f728832bfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cbd682db8d744e89616f29444499806",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d580271935fb4a49bfed2447b4262d8f",
            "value": 456318
          }
        },
        "08904a7147fe48f7bce477b64388526e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a0e8c2ad594499095eac569b85e3689",
            "placeholder": "​",
            "style": "IPY_MODEL_7f5a5b63fe144703b987f27cada63127",
            "value": " 456k/456k [00:00&lt;00:00, 57.3MB/s]"
          }
        },
        "97cd229f16b141108d2d680baa087c63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c14b19c10b944918b5a24d705a36abf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b035516ae4dc49a59fe20ad9c8c22e98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cbd682db8d744e89616f29444499806": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d580271935fb4a49bfed2447b4262d8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a0e8c2ad594499095eac569b85e3689": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f5a5b63fe144703b987f27cada63127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb050fd04a424ded806652d5975beece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f6743d40d5148158f00eefb90c9afa4",
              "IPY_MODEL_d894d4be75ef434a89568c8e37c77421",
              "IPY_MODEL_ce0fafc79c0a47da9058d3de58ebe6ac"
            ],
            "layout": "IPY_MODEL_c2eecf92b2b44e0e97317b190ce12e01"
          }
        },
        "9f6743d40d5148158f00eefb90c9afa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1066930225c14ea4b26fe9f7c7a61d5c",
            "placeholder": "​",
            "style": "IPY_MODEL_f0850f7d63d742ccb7f4f1c6ea775be3",
            "value": "tokenizer.json: 100%"
          }
        },
        "d894d4be75ef434a89568c8e37c77421": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b1b0b47e8f24328922bf53d1d71854f",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f40432cb167b4d0f978e22478b264052",
            "value": 1355863
          }
        },
        "ce0fafc79c0a47da9058d3de58ebe6ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9e2d79a06cb4bcc9b753e6cadcfe766",
            "placeholder": "​",
            "style": "IPY_MODEL_2ba6bc4b2462423a8a3a6c91b9330de7",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 14.2MB/s]"
          }
        },
        "c2eecf92b2b44e0e97317b190ce12e01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1066930225c14ea4b26fe9f7c7a61d5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0850f7d63d742ccb7f4f1c6ea775be3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b1b0b47e8f24328922bf53d1d71854f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f40432cb167b4d0f978e22478b264052": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d9e2d79a06cb4bcc9b753e6cadcfe766": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ba6bc4b2462423a8a3a6c91b9330de7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pettepiero/dl-nlp-exam/blob/main/Finetuning_Transformers_With_JAX_and_Haiku.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2XMaLX1mNhk"
      },
      "source": [
        "Just last month [DeepMind open-sourced Haiku](https://github.com/deepmind/dm-haiku), the JAX version of their tensorflow [neural network library Sonnet](https://github.com/deepmind/sonnet).  Today we'll be walking through a port of the RoBERTa pre-trained model to JAX + Haiku, then finetuning the model to solve a downstream task!\n",
        "\n",
        "If you're unfamiliar with JAX, need a primer on JAX fundamentals, or simply want to know why you should care, check out my previous post -- [A First Look at JAX](https://www.pragmatic.ml/first-look-at-jax/) or wander on over to the [official JAX documentation](https://jax.readthedocs.io/en/latest/).\n",
        "\n",
        "Some brief notes about the format of this blog post:\n",
        "    - This post will be code-oriented and show code examples first before providing commentary.\n",
        "    - We're going to be working in a top-down fashion, so we'll lay out broad strokes and then fill in the detail.\n",
        "    - Along the way I'll add notes on potential JAX / Haiku stumbling blocks and rough edges that you may find useful if you're just getting started with either.\n",
        "\n",
        "If you'd like to run the code as you read along I've also made this walkthrough available as a Colab notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7BDW9r5uQC5"
      },
      "source": [
        "Without further ado, let's explore Haiku!\n",
        "\n",
        "The Haiku README provides an excellent short description of Haiku's role in the JAX ecosystem:\n",
        "\n",
        "> * Haiku is a simple neural network library for JAX that enables users to use familiar object-oriented programming\n",
        "models while allowing full access to JAX's pure function transformations.\n",
        "> * Haiku provides two core tools: a module abstraction, hk.Module, and a simple function transformation, hk.transform.\n",
        "> * hk.Modules are Python objects that hold references to their own parameters, other modules, and methods that apply functions on user inputs.\n",
        "> * hk.transform turns functions that use these object-oriented, functionally \"impure\" modules into pure functions that can be used with jax.jit, jax.grad, jax.pmap, etc.\n",
        "\n",
        "In addition to those two key abstractions, Haiku provides a collection of common neural network components like convolutions, pooling operations, RNN / GRU / LSTM units, batch norm and other normalization methods, padding operations, and a suite of a initializers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYbj4qH_uRYe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b06d630d-cc88-4ab5-b103-cbda58c83c77"
      },
      "source": [
        "!pip install git+https://github.com/deepmind/dm-haiku transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/deepmind/dm-haiku\n",
            "  Cloning https://github.com/deepmind/dm-haiku to /tmp/pip-req-build-3_5mjroo\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/deepmind/dm-haiku /tmp/pip-req-build-3_5mjroo\n",
            "  Resolved https://github.com/deepmind/dm-haiku to commit d14f1b62798a4da47cf3243cd87b1ad7dc52a108\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from dm-haiku==0.0.14.dev0) (1.4.0)\n",
            "Collecting jmp>=0.0.2 (from dm-haiku==0.0.14.dev0)\n",
            "  Downloading jmp-0.0.4-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from dm-haiku==0.0.14.dev0) (1.26.4)\n",
            "Collecting tabulate>=0.8.9 (from dm-haiku==0.0.14.dev0)\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading jmp-0.0.4-py3-none-any.whl (18 kB)\n",
            "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Building wheels for collected packages: dm-haiku\n",
            "  Building wheel for dm-haiku (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dm-haiku: filename=dm_haiku-0.0.14.dev0-py3-none-any.whl size=373772 sha256=89b6d172d174edfb16f0ddaf1271d6c655a3e3acd845258e0f0d22aa7ffbecba\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-y1vu35fd/wheels/86/29/0a/454c478e7217c383ee3d05aa43cbeff48284c23430f59d0c4f\n",
            "Successfully built dm-haiku\n",
            "Installing collected packages: tabulate, jmp, dm-haiku\n",
            "Successfully installed dm-haiku-0.0.14.dev0 jmp-0.0.4 tabulate-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show jax"
      ],
      "metadata": {
        "id": "QAHVLQ49dxge",
        "outputId": "9bb627af-bea3-44a6-ed61-bd1adf868a6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: jax\n",
            "Version: 0.5.2\n",
            "Summary: Differentiate, compile, and transform Numpy code.\n",
            "Home-page: https://github.com/jax-ml/jax\n",
            "Author: JAX team\n",
            "Author-email: jax-dev@google.com\n",
            "License: Apache-2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: jaxlib, ml_dtypes, numpy, opt_einsum, scipy\n",
            "Required-by: chex, distrax, flax, optax, orbax-checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_d-yf46lTeo"
      },
      "source": [
        "import haiku as hk\n",
        "\n",
        "class Transformer(hk.Module):\n",
        "\n",
        "    def __init__(self, config, *args, **kwargs):\n",
        "        super().__init__(name=\"Transformer\")\n",
        "        self.config = config\n",
        "\n",
        "    def __call__(self, token_ids):\n",
        "        x = Embedding(config)(token_ids)\n",
        "        for layer_num, layer in enumerate(range(config.n_layers)):\n",
        "            x = TransformerBlock(config, layer_num=layer_num)(x)\n",
        "        return x\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LTWi6cDtJmb"
      },
      "source": [
        "First, a comment on how Haiku modules are typically structured.  The `__init__` function is where we manage module configuration -- storing values like the number of layers in our transformer, our hidden dimension size, and other parameters. In order for Haiku to function properly, the `__init__()` of all `hk.Modules` must call `super().__init__()`, optionally with a `name`.\n",
        "\n",
        "Note that if we wanted to we could have moved some of this logic around, constructing our TokenEmbedding object in the `__init__` method for instance. Although nothing prevents us from doing this, there are times when we don't have enough information to construct all our modules in the `__init__` method because our configuration is dependent on some property of our input.  Because of this, child `hk.Modules` are often constructed in the same method that is called when the module is applied to an input. It's also simply convenient to have the full declaration of module settings in line with the application of the module to an input, to prevent hopping back and forth between methods when reading code.\n",
        "\n",
        "In our example above, we use the `__call__` method as our application method for brevity but we could equivalently use `forward` or any other method name. An `hk.Module` can also expose more than one application method if desirable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDl06c3bVDcn"
      },
      "source": [
        "import haiku as hk\n",
        "\n",
        "class AlternateTransformer(hk.Module):\n",
        "    \"\"\"\n",
        "    An equally valid implementation\n",
        "    \"\"\"\n",
        "    def __init__(self, config, *args, **kwargs):\n",
        "        super().__init__(name=\"Transformer\")\n",
        "        self.config = config\n",
        "        self.embedder = Embedding(config)\n",
        "\n",
        "    def embed(self, tokens):\n",
        "        return self.embedder(tokens)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        x = self.embedder(tokens)\n",
        "        for layer in config.n_layers:\n",
        "            x = TransformerBlock(config)(x)\n",
        "        return x\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SYsJBfqVJdH"
      },
      "source": [
        "If you're interested in more detail on how `hk.Module` and `hk.transform` function, I'd recommend Sabrina Mielke's recent article, [\"From PyTorch to JAX: towards neural net frameworks that purify stateful code\"](https://sjmielke.com/jax-purify.htm), which builds up the motivation for the design decisions that Haiku made.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hre0n9grY50o"
      },
      "source": [
        "If we had already implemented the `Embedding` and `TransformerBlock` modules, we could convert our new `hk.Module` to a JAX compatible function through the use of `hk.transform`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPEc5qq3ZUJN"
      },
      "source": [
        "# We'll fill our our config later\n",
        "config = {'max_length': 512}\n",
        "\n",
        "def features(tokens):\n",
        "    transformer = Transformer(config)\n",
        "    return transformer(tokens)\n",
        "\n",
        "features_fn = hk.transform(features)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSvAcLL8azAk"
      },
      "source": [
        "Neat!  Let's write our `Embedding` module and plug in some pre-trained model weights from HuggingFace so we can start executing code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJP4q2debYHX"
      },
      "source": [
        "from transformers import RobertaModel\n",
        "import jax.numpy as jnp\n",
        "\n",
        "\n",
        "class Embedding(hk.Module):\n",
        "    \"\"\"\n",
        "    Embeds tokens and positions into an array of shape [n_batch, n_seq, n_hidden]\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "    def __call__(self, token_ids, training=False):\n",
        "        \"\"\"\n",
        "        token_ids: ints of shape (batch, n_seq)\n",
        "        \"\"\"\n",
        "        word_embeddings = self.config['pretrained']['embeddings/word_embeddings']\n",
        "\n",
        "        # We have to flatten our tokens before passing them to the hk.Embed module,\n",
        "        # as arrays with more than one dimension are interpreted as multi-dimensional indexes\n",
        "        flat_token_ids = jnp.reshape(token_ids, [token_ids.shape[0] * token_ids.shape[1]])\n",
        "        flat_token_embeddings = hk.Embed(\n",
        "            vocab_size=word_embeddings.shape[0],\n",
        "            embed_dim=word_embeddings.shape[1],\n",
        "\n",
        "            # Here we're using hk.initializers.Constant to supply pre-trained embeddings\n",
        "            # to our hk.Embed module\n",
        "            w_init=hk.initializers.Constant(self.config['pretrained']['embeddings/word_embeddings'])\n",
        "        )(flat_token_ids)\n",
        "\n",
        "        # After we've embedded our token IDs, we reshape to recover our batch dimension\n",
        "        token_embeddings = jnp.reshape(\n",
        "            flat_token_embeddings,\n",
        "            [token_ids.shape[0], token_ids.shape[1], word_embeddings.shape[1]]\n",
        "        )\n",
        "\n",
        "        # Combine our token embeddings with a set of learned positional embeddings\n",
        "        embeddings = token_embeddings + PositionEmbeddings(self.config)()\n",
        "        embeddings = hk.LayerNorm(\n",
        "            axis=-1,\n",
        "            create_scale=True,\n",
        "            create_offset=True,\n",
        "\n",
        "            # The layer norm parameters are also pretrained, so we have to take care to\n",
        "            # use a constant initializer for these as well\n",
        "            scale_init=hk.initializers.Constant(\n",
        "                self.config['pretrained']['embeddings/LayerNorm/gamma']\n",
        "            ),\n",
        "            offset_init=hk.initializers.Constant(\n",
        "                self.config['pretrained']['embeddings/LayerNorm/beta']\n",
        "            )\n",
        "        )(embeddings)\n",
        "\n",
        "        # Dropout is will be applied later when we finetune our Roberta implementation\n",
        "        # to solve a classification task. For now we'll set `training` to False.\n",
        "        if training:\n",
        "            embeddings = hk.dropout(\n",
        "                # Haiku magic -- we'll explicitly provide a RNG key to haiku later to make this function\n",
        "                hk.next_rng_key(),\n",
        "                rate=self.config['embed_dropout_rate'],\n",
        "                x=embeddings\n",
        "            )\n",
        "\n",
        "        return embeddings"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2Rm8LL3cE2d"
      },
      "source": [
        "**Our** Embedding module is pretty straightforward:\n",
        "\n",
        "*   Using `hk.Embed`, we perform a lookup to get the vector that corresponds to each of our token IDs.\n",
        "    * We initialize this to the pre-trained embedding matrix we downloaded.\n",
        "*   We add `PositionEmbeddings()` -- a `hk.Module` we have yet to define.  Assuming a fixed sequence length, this is nothing more than a static matrix that we broadcast to each sequence in our batch.\n",
        "*   We normalize our embeddings using layer norm, applying the scales and offsets learned during pre-training.\n",
        "*   Finally, we optionally apply dropout to our embeddings at train time.\n",
        "    * Note the hk.next_rng_key() feature. With vanilla JAX, we would have to make sure to pass our `PRNGKey` around to every module that needs it.  A handy feature of Haiku is that your `PRNGKey` is exposed via the `hk.next_rng_key` utility with the context of `hk.transform`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuXwMMNjcEP_"
      },
      "source": [
        "\n",
        "class PositionEmbeddings(hk.Module):\n",
        "    \"\"\"\n",
        "    A position embedding of shape [n_seq, n_hidden]\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        # The Roberta position embeddings are offset by 2\n",
        "        self.offset = 2\n",
        "\n",
        "    def __call__(self):\n",
        "        pretrained_position_embedding = self.config['pretrained']['embeddings/position_embeddings']\n",
        "        position_weights = hk.get_parameter(\n",
        "            \"position_embeddings\",\n",
        "            pretrained_position_embedding.shape,\n",
        "            init=hk.initializers.Constant(pretrained_position_embedding)\n",
        "        )\n",
        "\n",
        "        return position_weights[self.offset:self.offset + self.config['max_length']]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w83Tfx2efQl"
      },
      "source": [
        "The `PositionEmbeddings` module is also pretty straightforward -- we simply slice the pre-trained position embedding matrix to the desired length and return it. However, our `PositionEmbeddings` has introduced a new key haiku function!\n",
        "\n",
        "```\n",
        "hk.get_parameter(name, shape, dtype, init)\n",
        "```\n",
        "\n",
        "The `hk.get_parameter` function is how Haiku keeps track of parameter state for us.  The module we're in and the `name` argument passed to `hk.get_parameter` register this new parameter in a Haiku-managed parameter store. If we were writing something similar with vanilla JAX, we would have to keep track of these parameters manually. Later on we'll see how `hk.transform` allows us to retrieve the values of our parameters using `init()`.\n",
        "\n",
        "[Extended docs for `hk.get_parameter`](https://dm-haiku.readthedocs.io/en/latest/api.html#haiku.get_parameter) are available in the [official Haiku documentation](https://dm-haiku.readthedocs.io/en/latest/).\n",
        "\n",
        "Now that you're familiar with `hk.get_parameter`, let's load in some pre-trained model weights so we can test out what we've put together so far."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4hScrweZlnM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368,
          "referenced_widgets": [
            "a93f7dec8e1b480ebe5f223f3bb01aa4",
            "9fef59ee481449a4bc7b0830c31427e6",
            "356b14e619e0427abdfa700af086362e",
            "a5dff17b296d458eaeb3f3974d5c1005",
            "b9d893e8265e46e0b051e4f264f80dd4",
            "7fcae41536004881b5724a65e4ca6031",
            "0320177245c84174b390632a70d5a351",
            "ae156a0e05d94a6792f4687b6ce4ca2a",
            "01a1f0dd89c7401eab2a8d1a4ecf9920",
            "8cc945b0b7a64a0c94c74f287f1f789a",
            "e376bd1b0ee2455aa838084e94720c99",
            "f493a4cebb7a4efda5d4538672f0024b",
            "0a47e6c3625d44979a68efeaa3ecb495",
            "f77e52f619db4ef59fe9f231c25c3ce0",
            "6183b6c9d464493b84d622aaa8f12a15",
            "6c7bc8bd014d44a6b47f9d2358656d66",
            "3ea17799985245f0b7dafdc5f615da27",
            "ada6e035c9544e8aba3a50374c048663",
            "b71f90c1247c44f3a054b05f13419c50",
            "206418e04e8b480c974dcf925043d788",
            "65218e3199a74b339bedecea52d34a30",
            "15ccd38e2027443da07631a3a81bf69d",
            "dd50da6beabf44feb46cad4dd2595535",
            "4a489fb803764f17814cd54c90c6c7d3",
            "7faed7d01a9d49e9a1ca1904106e7189",
            "3245721ecd864b09a7470bf8b41b751c",
            "57854a38cb5748dfb532fd446dfabbdc",
            "f51cd4848c03423daa1971fbc1d8c160",
            "6c7f7d72f81a4df6883bd5b664313136",
            "35f72693432f419d815aa111344bfd3c",
            "abe2e33a918d408a9b0f988fb6ae5451",
            "a29de0629865458a8bc55ab12620171c",
            "c2bf313a92ef4b389ed2fcfd5c201679",
            "186a9ef2e10b41f39c7cee4a9c61d5ef",
            "afd9f15f55744bf0adcf972be8b7cf3e",
            "36c328f206e04100be4fb0041daae4c7",
            "ed0616ce2a5c41ad8f33539a82a254db",
            "eddc617c6c3d45809f6a2ff260b888e8",
            "652249ef6b0c4c95852cfb345d3dda30",
            "31fd72607b364202a322694f2c056d50",
            "d394c6b488ba4ab18a9d8e94999e9559",
            "28879a5078134661957094a4241a097e",
            "a77bac0ebbf14c1b9173cefe99346207",
            "7c5170db10094ff494f2b4980a02bbef",
            "e0aa1ec75b474e0ebb64ca7bb2c3f847",
            "fd76ee3edbd54a8bb06f08ef2f7823f4",
            "6512e14a8af14979989be4f728832bfc",
            "08904a7147fe48f7bce477b64388526e",
            "97cd229f16b141108d2d680baa087c63",
            "3c14b19c10b944918b5a24d705a36abf",
            "b035516ae4dc49a59fe20ad9c8c22e98",
            "3cbd682db8d744e89616f29444499806",
            "d580271935fb4a49bfed2447b4262d8f",
            "1a0e8c2ad594499095eac569b85e3689",
            "7f5a5b63fe144703b987f27cada63127",
            "fb050fd04a424ded806652d5975beece",
            "9f6743d40d5148158f00eefb90c9afa4",
            "d894d4be75ef434a89568c8e37c77421",
            "ce0fafc79c0a47da9058d3de58ebe6ac",
            "c2eecf92b2b44e0e97317b190ce12e01",
            "1066930225c14ea4b26fe9f7c7a61d5c",
            "f0850f7d63d742ccb7f4f1c6ea775be3",
            "5b1b0b47e8f24328922bf53d1d71854f",
            "f40432cb167b4d0f978e22478b264052",
            "d9e2d79a06cb4bcc9b753e6cadcfe766",
            "2ba6bc4b2462423a8a3a6c91b9330de7"
          ]
        },
        "outputId": "fff85d14-09fa-432f-b4a8-ca7f43ddbdd6"
      },
      "source": [
        "from io import BytesIO\n",
        "from functools import lru_cache\n",
        "\n",
        "import joblib\n",
        "import requests\n",
        "\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "\n",
        "# We'll make use of these again later as a means to check our implementation\n",
        "huggingface_roberta = RobertaModel.from_pretrained('roberta-base', output_hidden_states=True)\n",
        "huggingface_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Some light postprocessing to make the parameter keys a bit more concise\n",
        "def postprocess_key(key):\n",
        "    return key.replace('model/featurizer/bert/', '').replace(':0', '').replace('self/', '')\n",
        "\n",
        "\n",
        "# Cache the downloaded file to go easy on the tubes\n",
        "@lru_cache()\n",
        "def get_pretrained_weights():\n",
        "    # We'll use the weight dictionary from the Roberta encoder at https://github.com/IndicoDataSolutions/finetune\n",
        "    remote_url = \"https://bendropbox.s3.amazonaws.com/roberta/roberta-model-sm-v2.jl\"\n",
        "    weights = joblib.load(BytesIO(requests.get(remote_url).content))\n",
        "\n",
        "    weights = {\n",
        "        postprocess_key(key): value\n",
        "        for key, value in weights.items()\n",
        "    }\n",
        "\n",
        "    # We use huggingface's word embedding matrix because their token IDs mapping varies slightly from the\n",
        "    # format used in our joblib file above\n",
        "    weights['embeddings/word_embeddings'] = (\n",
        "        huggingface_roberta.get_input_embeddings().weight.detach().numpy()\n",
        "    )\n",
        "    return weights\n",
        "\n",
        "\n",
        "class Scope(object):\n",
        "    \"\"\"\n",
        "    A tiny utility to help make looking up into our dictionary cleaner.\n",
        "    There's no haiku magic here.\n",
        "    \"\"\"\n",
        "    def __init__(self, weights, prefix):\n",
        "        self.weights = weights\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        return self.weights[self.prefix + key]\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a93f7dec8e1b480ebe5f223f3bb01aa4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f493a4cebb7a4efda5d4538672f0024b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd50da6beabf44feb46cad4dd2595535"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "186a9ef2e10b41f39c7cee4a9c61d5ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0aa1ec75b474e0ebb64ca7bb2c3f847"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb050fd04a424ded806652d5975beece"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPr8IW9Lkf1b",
        "outputId": "e6a16844-aef7-4aa4-dd6c-638ecb0f9f9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pretrained = get_pretrained_weights()\n",
        "print([k for k in pretrained.keys() if 'embedding' in k])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['embeddings/word_embeddings', 'embeddings/LayerNorm/beta', 'embeddings/LayerNorm/gamma', 'embeddings/token_type_embeddings', 'embeddings/position_embeddings']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HedW0FSl56u"
      },
      "source": [
        "Looks like we have the weights we need for our `Embedding` module, but we're still missing a way to go from text to numeric token IDs in our word embedding matrix.  Let's load in the pre-written tokenizer from huggingface to save ourselves some pain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfdSTXulmQHR",
        "outputId": "b2c3f87c-831d-4029-c21a-55d25e995144",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sample_text = \"This was a lot less painful than re-implementing a tokenizer\"\n",
        "encoded = huggingface_tokenizer.batch_encode_plus(\n",
        "    [sample_text, sample_text],\n",
        "    pad_to_max_length=True,\n",
        "    max_length=config['max_length']\n",
        ")\n",
        "sample_tokens = encoded['input_ids']\n",
        "print(sample_tokens[0][:20])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 713, 21, 10, 319, 540, 8661, 87, 769, 12, 757, 40224, 154, 10, 19233, 6315, 2, 1, 1, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2681: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLbi5eXLnsUM"
      },
      "source": [
        "Looks good!  We've passed the `pad_to_max_length` and `max_length` arguments so that the `huggingface_tokenizer` can handle padding out sequences to a constant length for us -- and it's clearly working as the sea of 1's above show us. Token ID 1 corresponds to the padding token used by RoBERTa.\n",
        "\n",
        "Now we have all the necessary ingredients to test out our embedding layer, let's `hk.transform` the embedding operation and take things for a spin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5XQBdLfn_r2",
        "outputId": "5bf6b957-ca0b-422e-be13-9d5578f908c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from jax import jit\n",
        "from jax.random import PRNGKey\n",
        "import numpy as np\n",
        "\n",
        "config = {\n",
        "    'pretrained': pretrained,\n",
        "    'max_length': 512,\n",
        "    'embed_dropout_rate': 0.1\n",
        "}\n",
        "\n",
        "def embed_fn(tokens, training=False):\n",
        "    embedding = Embedding(config)(tokens)\n",
        "    return embedding\n",
        "\n",
        "rng = PRNGKey(42)\n",
        "embed = hk.transform(embed_fn, apply_rng=True)\n",
        "sample_tokens = np.asarray(sample_tokens)\n",
        "params = embed.init(rng, sample_tokens, training=False)\n",
        "embedded_tokens = jit(embed.apply)(params, rng, sample_tokens)\n",
        "print(embedded_tokens.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 512, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNkJ5Ruqy3f3"
      },
      "source": [
        "\n",
        "With that, we've successfully executed our first snippet of code using Haiku!\n",
        "\n",
        "First we wrote `embed_fn` to instantiate an instance of our `Embedding` module and call it.  This is necessary to wrap up all the requisite state for the embedding function for haiku.\n",
        "\n",
        "If you accidentally try to instantiate a `hk.Module` outside of a `hk.transform` context you'll receive a helpful reminder that this isn't permitted. Haiku imposes this constraint so it can \"purify\" our stateful `hk.Module` and convert it into a pure function that functions with JAX.\n",
        "\n",
        "```\n",
        "embedding = Embedding(config)\n",
        "```\n",
        "\n",
        "```\n",
        "ValueError: All `hk.Module`s must be initialized inside an `hk.transform`.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljzB0vhx3L-R"
      },
      "source": [
        "You've probably also noticed that we're using two methods we haven't yet spoken about -- `init()` and `apply()`.  \n",
        "\n",
        "```\n",
        "params = embed.init(rng, sample_tokens, training=False)\n",
        "```\n",
        "The `init` method is a haiku utility that gathers up all the parameters of our custom haiku modules for us and consolidates them into a `frozendict`. The `init()` method is also responsible for initializing any unitialized parameters, which is why we pass a source of randomness (our `rng`) as the first argument.\n",
        "\n",
        "Let's inspect our `params` variable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vytGcmMS3q9v",
        "outputId": "b1cd8c4a-5524-43b0-c06d-50f08a8248cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print({key: type(value) for key, value in params.items()})"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'embedding/embed': <class 'dict'>, 'embedding/position_embeddings': <class 'dict'>, 'embedding/layer_norm': <class 'dict'>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLGZ-4Zb3_Zh"
      },
      "source": [
        "When we use a `hk.Module` within another `hk.Module`, it gets placed in a subdictionary.  But if we drill down into our frozendict we'll eventually hit the bottom and uncover an `np.ndarray` -- the current state of the weights of our model.  If you're familiar with tensorflow's concept of a variable scope, this should feel familiar.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsyBSKQx4hwZ",
        "outputId": "dd40586e-0670-42ff-8b5f-876a1e3cd5ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print({key: type(value) for key, value in params['embedding/layer_norm'].items()})"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'scale': <class 'jaxlib.xla_extension.ArrayImpl'>, 'offset': <class 'jaxlib.xla_extension.ArrayImpl'>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNyITBR24kkm"
      },
      "source": [
        "It's worth noting, however, that we need to use haiku's `hk.get_parameter` construct (or a `hk.Module`) for the parameters to be tracked automatically.  If we try to use a simple `jnp.ndarray` within the context of `hk.transform` it won't be tracked as a trainable parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0judoXS443VS",
        "outputId": "78e868b0-c5c8-481e-fa77-ef2754db037e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import jax\n",
        "\n",
        "def linear_fn(x):\n",
        "    w = jax.random.normal(hk.next_rng_key(), (10, 10))\n",
        "    b = jnp.zeros(10)\n",
        "    return np.dot(w, x) + b\n",
        "\n",
        "linear = hk.transform(linear_fn)\n",
        "params = linear.init(PRNGKey(0), np.random.rand(10))\n",
        "print(params.keys())\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ec4Z-UcJJKu"
      },
      "source": [
        "The second new method is `.apply()`:\n",
        "\n",
        "```\n",
        "embedded_tokens = embed.apply(params, rng, sample_tokens)\n",
        "```\n",
        "\n",
        "The `apply()` method injects the parameters of our model (and if we've passed `apply_rng` to `hk.transform`, our pseudo-random number generator) to the embed function so that we have all the necessary state to compute the output of our function.  This pattern is how `hk.transform` is able to turn a stateful `hk.Module` class into a JAX compatible operation -- `init()` and `apply()` are natural counterparts. Our `init()` method extracts the problematic state from our module, and the second passes that state back into the pure `apply()` method.  Because `apply()` is functionally pure, we're totally free to compose it with any of the standard JAX operations like `jit()` and `grad()`!\n",
        "\n",
        "```\n",
        "embedded_tokens = jit(embed.apply)(params, rng, sample_tokens)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8WC81XVMqoA"
      },
      "source": [
        "In all honestly, that's about all there is to working with Haiku!  That's part of the beauty of it -- Haiku aims to be a library, not a framework -- providing a suite of utilities that complement JAX but doing it's best not to get in your way by requiring custom formats or imposing problematic constraints.\n",
        "\n",
        "With the critical pieces of the haiku API behind us, let's continue implementing our transformer!  Next up -- the `TransformerBlock` module.\n",
        "\n",
        "![alt text](https://i.imgur.com/UrXOmY7.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXOt56z-Rowl"
      },
      "source": [
        "class TransformerBlock(hk.Module):\n",
        "\n",
        "    def __init__(self, config, layer_num):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.n = layer_num\n",
        "\n",
        "    def __call__(self, x, mask, training=False):\n",
        "        scope = Scope(self.config['pretrained'], f'encoder/layer_{self.n}/')\n",
        "        # Feed our input through a multi-head attention operation\n",
        "        attention_output = MultiHeadAttention(self.config, self.n)(x, mask, training=training)\n",
        "\n",
        "        # Add a residual connection with the input to the layer\n",
        "        residual = attention_output + x\n",
        "\n",
        "        # Apply layer norm to the combined output\n",
        "        attention_output = hk.LayerNorm(\n",
        "            axis=-1,\n",
        "            create_scale=True,\n",
        "            create_offset=True,\n",
        "            scale_init=hk.initializers.Constant(scope['attention/output/LayerNorm/gamma']),\n",
        "            offset_init=hk.initializers.Constant(scope['attention/output/LayerNorm/beta']),\n",
        "        )(residual)\n",
        "\n",
        "        # Project out to a larger dim, apply a gelu, and then project back down to our hidden dim\n",
        "        mlp_output = TransformerMLP(self.config, self.n)(attention_output, training=training)\n",
        "\n",
        "        # Residual connection to the output of the attention operation\n",
        "        output_residual = mlp_output + attention_output\n",
        "\n",
        "        # Apply another LayerNorm\n",
        "        layer_output = hk.LayerNorm(\n",
        "            axis=-1,\n",
        "            create_scale=True,\n",
        "            create_offset=True,\n",
        "            scale_init=hk.initializers.Constant(scope['output/LayerNorm/gamma']),\n",
        "            offset_init=hk.initializers.Constant(scope['output/LayerNorm/beta']),\n",
        "        )(output_residual)\n",
        "        return layer_output"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmiwUBmH8sx3"
      },
      "source": [
        "We feed the inputs to each transformer block through it's signature self-attention layer, then add in residuals and apply layer normalization.\n",
        "We then feed the attention outputs through a 2 layer MLP, apply the residuals from the self-attention output, and apply a second layer normalization step.\n",
        "\n",
        "Let's define our MultiHeadAttention layer next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsusUYin4sQW"
      },
      "source": [
        "![Multi-Head Self-Attention](https://i.imgur.com/FvjWVvX.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzLCMjzOZX-W"
      },
      "source": [
        "class MultiHeadAttention(hk.Module):\n",
        "\n",
        "    def __init__(self, config, layer_num):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.n = layer_num\n",
        "\n",
        "    def _split_into_heads(self, x):\n",
        "        return jnp.reshape(\n",
        "            x,\n",
        "            [\n",
        "                x.shape[0],\n",
        "                x.shape[1],\n",
        "                self.config['n_heads'],\n",
        "                x.shape[2] // self.config['n_heads']\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __call__(self, x, mask, training=False):\n",
        "        \"\"\"\n",
        "        x: tensor of shape (batch, seq, n_hidden)\n",
        "        mask: tensor of shape (batch, seq)\n",
        "        \"\"\"\n",
        "        scope = Scope(self.config['pretrained'], f'encoder/layer_{self.n}/attention/')\n",
        "\n",
        "        # Project to queries, keys, and values\n",
        "        # Shapes are all [batch, sequence_length, hidden_size]\n",
        "        queries = hk.Linear(\n",
        "            output_size=self.config['hidden_size'],\n",
        "            w_init=hk.initializers.Constant(scope['query/kernel']),\n",
        "            b_init=hk.initializers.Constant(scope['query/bias'])\n",
        "        )(x)\n",
        "        keys = hk.Linear(\n",
        "            output_size=self.config['hidden_size'],\n",
        "            w_init=hk.initializers.Constant(scope['key/kernel']),\n",
        "            b_init=hk.initializers.Constant(scope['key/bias'])\n",
        "        )(x)\n",
        "        values = hk.Linear(\n",
        "            output_size=self.config['hidden_size'],\n",
        "            w_init=hk.initializers.Constant(scope['value/kernel']),\n",
        "            b_init=hk.initializers.Constant(scope['value/bias'])\n",
        "        )(x)\n",
        "\n",
        "        # Reshape our hidden state to group into heads\n",
        "        # New shape are [batch, sequence_length, n_heads, size_per_head]\n",
        "        queries = self._split_into_heads(queries)\n",
        "        keys = self._split_into_heads(keys)\n",
        "        values = self._split_into_heads(values)\n",
        "\n",
        "\n",
        "        # Compute per head attention weights\n",
        "        # b: batch\n",
        "        # s: source sequence\n",
        "        # t: target sequence\n",
        "        # n: number of heads\n",
        "        # h: per-head hidden state\n",
        "\n",
        "        # Note -- we could also write this with jnp.reshape and jnp.matmul, but I'm becoming\n",
        "        # a fan of how concise opting to use einsum notation for this kind of operation is.\n",
        "        # For more info, see https://rockt.github.io/2018/04/30/einsum and\n",
        "        # https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/\n",
        "        attention_logits = jnp.einsum('bsnh,btnh->bnst', queries, keys) / np.sqrt(queries.shape[-1])\n",
        "        # Add logits of mask tokens with a large negative number to prevent attending to those terms.\n",
        "        #attention_logits += jnp.reshape(mask * -2**32, [mask.shape[0], 1, 1, mask.shape[1]])\n",
        "        attention_logits += jnp.reshape(mask * -2**16, [mask.shape[0], 1, 1, mask.shape[1]])\n",
        "        # Instead of -2**32, use -1e9 or -jnp.inf which are handled safely by softmax\n",
        "        #attention_logits = jnp.where(mask[..., None, :], -1e9, attention_logits)\n",
        "        attention_weights = jax.nn.softmax(attention_logits, axis=-1)\n",
        "        per_head_attention_output = jnp.einsum('btnh,bnst->bsnh', values, attention_weights)\n",
        "        attention_output = jnp.reshape(\n",
        "            per_head_attention_output,\n",
        "            [\n",
        "                per_head_attention_output.shape[0],\n",
        "                per_head_attention_output.shape[1],\n",
        "                per_head_attention_output.shape[2] * per_head_attention_output.shape[3]\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Apply dense layer to output of attention operation\n",
        "        attention_output = hk.Linear(\n",
        "            output_size=self.config['hidden_size'],\n",
        "            w_init=hk.initializers.Constant(scope['output/dense/kernel']),\n",
        "            b_init=hk.initializers.Constant(scope['output/dense/bias'])\n",
        "        )(attention_output)\n",
        "\n",
        "        # Apply dropout at training time\n",
        "        if training:\n",
        "            attention_output = hk.dropout(\n",
        "                rng=hk.next_rng_key(),\n",
        "                rate=self.config['attention_drop_rate'],\n",
        "                x=attention_output\n",
        "            )\n",
        "\n",
        "        return attention_output"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldz2kmfp8sx7"
      },
      "source": [
        "\n",
        "We project our hidden state out to key, query, and value tensors of the same dimensions as the input hidden state. We then reshape the last dimension of our matrix to group neighboring activations into N heads. We dot our queries and keys to produce a measure of agreement that we'll use as an attention logit, divide by the square root of our head size as a way to prevent the attention distribution from being too sharp, and then apply our softmax to produce our attention weights.  Our then use our attention weights in conjunction with our values to produce our new hidden state, and reshape our matrices to re-combine the heads.  Finally, we apply a linear projection to our attention outputs and optionally apply dropout at training time.\n",
        "\n",
        "We won't spent too much time on the details here, as the intent of this blog post is to highlight using JAX and Haiku rather than devote too much time to an explanation of self-attention.\n",
        "\n",
        "If you'd like a more indepth refresher on the self-attention operation, I'd recommend:\n",
        "\n",
        "- [Alex Rush's Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html#attention)\n",
        "- [Jay Alammar's Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBXoQyZe_r-d"
      },
      "source": [
        "![alt text](https://i.imgur.com/4hDiOaa.pngps://)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAHS3EpcaHQ1"
      },
      "source": [
        "\n",
        "def gelu(x):\n",
        "    \"\"\"\n",
        "    We use this in place of jax.nn.relu because the approximation used\n",
        "    produces a non-trivial difference in the output state\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + jax.scipy.special.erf(x / jnp.sqrt(2.0)))\n",
        "\n",
        "\n",
        "class TransformerMLP(hk.Module):\n",
        "\n",
        "    def __init__(self, config, layer_num):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.n = layer_num\n",
        "\n",
        "    def __call__(self, x, training=False):\n",
        "        # Project out to higher dim\n",
        "        scope = Scope(self.config['pretrained'], f'encoder/layer_{self.n}/')\n",
        "        intermediate_output = hk.Linear(\n",
        "            output_size=self.config['intermediate_size'],\n",
        "            w_init=hk.initializers.Constant(scope['intermediate/dense/kernel']),\n",
        "            b_init=hk.initializers.Constant(scope['intermediate/dense/bias'])\n",
        "        )(x)\n",
        "\n",
        "        # Apply gelu nonlinearity\n",
        "        intermediate_output = gelu(intermediate_output)\n",
        "\n",
        "        # Project back down to hidden size\n",
        "        output = hk.Linear(\n",
        "            output_size=self.config['hidden_size'],\n",
        "            w_init=hk.initializers.Constant(scope['output/dense/kernel']),\n",
        "            b_init=hk.initializers.Constant(scope['output/dense/bias']),\n",
        "        )(intermediate_output)\n",
        "\n",
        "        # Apply dropout at training time\n",
        "        if training:\n",
        "            output = hk.dropout(\n",
        "                rng=hk.next_rng_key(),\n",
        "                rate=self.config['fully_connected_drop_rate'],\n",
        "                x=output\n",
        "            )\n",
        "\n",
        "        return output\n",
        ""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-8HJFjN8syA"
      },
      "source": [
        "The final component of our pre-trained RoBERTa model is the Transformer MLP block.  The linear MLP block contains a linear up-projection from our hidden size to a larger intermediate hidden representation, the application of a single gaussian error linear unit (GELU), and a linear projection back down to our hidden size. As per usual, we optionally apply dropout at training time.\n",
        "\n",
        "In the code block below, we wrap up all our previous work, embedding our input token IDs with our `Embedding` module and applying 12 layers of our `TransformerBlock`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83ikp-ZeAF6h"
      },
      "source": [
        "import haiku as hk\n",
        "\n",
        "class RobertaFeaturizer(hk.Module):\n",
        "    def __init__(self, config, *args, **kwargs):\n",
        "        super().__init__(name=\"Transformer\")\n",
        "        self.config = config\n",
        "\n",
        "    def __call__(self, token_ids, training=False):\n",
        "        x = Embedding(self.config)(token_ids, training=training)\n",
        "        mask = (token_ids == self.config['mask_id']).astype(jnp.float32)\n",
        "        for layer_num, layer in enumerate(range(config['n_layers'])):\n",
        "            x = TransformerBlock(config, layer_num=layer_num)(x, mask, training=training)\n",
        "        return x\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCSgLNxR8syD"
      },
      "source": [
        "With that final `hk.Module` complete, we'll populate the config object we've been referencing through our `hk.Module`'s and apply `hk.transform` to produce a pure function. Even without attaching a classification head to the pre-trained base, we already have features we could use for purposes of computing textual similarities or similar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRwclaIt-t6K",
        "outputId": "85dcfb6e-f3db-4ad3-d95c-f1df1dbcb41b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from jax import jit\n",
        "from jax.random import PRNGKey\n",
        "\n",
        "config = {\n",
        "    'pretrained': pretrained,\n",
        "    'max_length': 512,\n",
        "    'embed_dropout_rate': 0.1,\n",
        "    'fully_connected_drop_rate': 0.1,\n",
        "    'attention_drop_rate': 0.1,\n",
        "    'hidden_size': 768,\n",
        "    'intermediate_size': 3072,\n",
        "    'n_heads': 12,\n",
        "    'n_layers': 12,\n",
        "    'mask_id': 1,\n",
        "    'weight_stddev': 0.02,\n",
        "\n",
        "    # For use later in finetuning\n",
        "    'n_classes': 2,\n",
        "    'classifier_drop_rate': 0.1,\n",
        "    'learning_rate': 1e-5,\n",
        "    'max_grad_norm': 1.0,\n",
        "    'l2': 0.1,\n",
        "    'n_epochs': 5,\n",
        "    'batch_size': 4\n",
        "}\n",
        "\n",
        "def featurizer_fn(tokens, training=False):\n",
        "    contextual_embeddings = RobertaFeaturizer(config)(tokens, training=training)\n",
        "    return contextual_embeddings\n",
        "\n",
        "rng = PRNGKey(42)\n",
        "roberta = hk.transform(featurizer_fn, apply_rng=True)\n",
        "sample_tokens = np.asarray(sample_tokens)\n",
        "params = roberta.init(rng, sample_tokens, training=False)\n",
        "contextual_embedding = jit(roberta.apply)(params, rng, sample_tokens)\n",
        "print(contextual_embedding.shape)\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 512, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAPxOHn88XXz"
      },
      "source": [
        "Let's check to make sure our implementation matches up with a known functional implementation -- we'll opt to use the hugging face model we instantiated earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8qbpwGzEPA1",
        "outputId": "0d4e702c-5495-48cc-e7b9-f75f199443d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "import torch\n",
        "batch_token_ids = torch.tensor(huggingface_tokenizer.encode(sample_text)).unsqueeze(0)\n",
        "huggingface_output_state, huggingface_pooled_state, _ = huggingface_roberta.forward(batch_token_ids)\n",
        "print(np.allclose(\n",
        "    huggingface_output_state.detach().numpy(),\n",
        "    contextual_embedding[:1, :batch_token_ids.size()[1]],\n",
        "    atol=1e-3\n",
        "))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'detach'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-fe34b0dda339>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhuggingface_output_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuggingface_pooled_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhuggingface_roberta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_token_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m print(np.allclose(\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mhuggingface_output_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mcontextual_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mbatch_token_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'detach'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIAdWiaJ8qBg"
      },
      "source": [
        "Great!  The contextual embeddings line up, so our implementation is correct.\n",
        "\n",
        "Admittedly, the first time I ran this things didn't go quite so smoothly -- I missed the subtle difference in the two `gelu` implementations and the difference in outputs was enough to make this check fail.  We can't directly inspect the intermediate outputs of our functions if we have things wrapped in a `jit` call like our example above, but if you remove the `jit` call to `roberta.apply`, it's trivial to add print statements to our implementation to track activations at intermediate points in our network and compare to the Hugging Face implementation.  This is a common gotcha if you're new to JAX, and I recommend reading [JAX core developer Matthew Johnson's response to a github issue](https://github.com/google/jax/issues/196#issuecomment-451671635) on the topic of printing within `jit` if you're curious why this limitation exists.\n",
        "\n",
        "Now that our featurizer is implemented, let's wrap this up to use for downstream classification tasks!\n",
        "This is as easy as slicing off the hidden state of the first token and applying a linear projection.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx2vIBdvCsNQ"
      },
      "source": [
        "class RobertaClassifier(hk.Module):\n",
        "\n",
        "    def __init__(self, config, *args, **kwargs):\n",
        "        super().__init__(name=\"Transformer\")\n",
        "        self.config = config\n",
        "\n",
        "    def __call__(self, token_ids, training=False):\n",
        "        sequence_features = RobertaFeaturizer(self.config)(token_ids=token_ids, training=training)\n",
        "\n",
        "        # Our classifier representation is just the output state of our first token\n",
        "        clf_state = sequence_features[:,0,:]\n",
        "\n",
        "        if training:\n",
        "            clf_state = hk.dropout(\n",
        "                rng=hk.next_rng_key(),\n",
        "                rate=self.config['classifier_drop_rate'],\n",
        "                x=clf_state\n",
        "            )\n",
        "\n",
        "        # We project down from our hidden dimension to n_classes and use this as our softmax logits\n",
        "        clf_logits = hk.Linear(\n",
        "            output_size=self.config['n_classes'],\n",
        "            w_init=hk.initializers.TruncatedNormal(self.config['weight_stddev'])\n",
        "        )(clf_state)\n",
        "\n",
        "        return clf_logits\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT8njBXlBZJ1"
      },
      "source": [
        "Let's plug in a real dataset to try it out.  As much as I dislike the trope of testing text classifiers on sentiment analysis, we'll be using the IMDB Sentiment dataset from tensorflow datasets because it's already packaged up neatly for us.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS-SWhz8CnpI"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "\n",
        "def load_dataset(split, training, batch_size, n_epochs=1, n_examples=None):\n",
        "    \"\"\"Loads the dataset as a generator of batches.\"\"\"\n",
        "    ds = tfds.load(\"imdb_reviews\", split=f\"{split}[:{n_examples}]\").cache().repeat(n_epochs)\n",
        "    if training:\n",
        "        ds = ds.shuffle(10 * batch_size, seed=0)\n",
        "    ds = ds.batch(batch_size)\n",
        "    return tfds.as_numpy(ds)\n",
        "\n",
        "n_examples = 25000\n",
        "train = load_dataset(\"train\", training=True, batch_size=4, n_epochs=config['n_epochs'], n_examples=n_examples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fSjd-MZCwVz"
      },
      "source": [
        "We'll add in an `encode_batch` utility to make calling the huggingface tokenizer more concise, transformer our new `RobertaClassifier` module into a pure function, and initialize our model state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4MMySIgC6rh"
      },
      "source": [
        "from jax.experimental import optix\n",
        "\n",
        "def roberta_classification_fn(batch_token_ids, training):\n",
        "    model = RobertaClassifier(config)(\n",
        "        jnp.asarray(batch_token_ids),\n",
        "        training=training\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def encode_batch(batch_text):\n",
        "    # Accept either utf-8 encoded bytes or unicode\n",
        "    batch_text = [\n",
        "        text.decode('utf-8') if isinstance(text, bytes) else text\n",
        "        for text in batch_text\n",
        "    ]\n",
        "\n",
        "    # Use huggingface's tokenizer to convert from raw text to integer token ids\n",
        "    token_ids = huggingface_tokenizer.batch_encode_plus(\n",
        "        batch_text,\n",
        "        pad_to_max_length=True,\n",
        "        max_length=config['max_length'],\n",
        "    )['input_ids']\n",
        "    return np.asarray(token_ids)\n",
        "\n",
        "\n",
        "# Purify our RobertaClassifier through the use of hk.transform and initialize our classifier\n",
        "rng = jax.random.PRNGKey(42)\n",
        "roberta_classifier = hk.transform(roberta_classification_fn, apply_rng=True)\n",
        "params = roberta_classifier.init(\n",
        "    rng,\n",
        "    batch_token_ids=encode_batch(['Sample text', 'Sample text']),\n",
        "    training=True\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13FkGz158syX"
      },
      "source": [
        "Next we jit compile some functions that use our roberta classifier for computing the loss, measuring model accuracy, and computing gradient updates.\n",
        "Because passed `apply_rng=True` to the call to `hk.transform`, the first arguments to `roberta_classifier.apply` are `params` and `rng`.  After the positional haiku arguments we supply the rest of the arguments our transformed roberta_classifier function expects.\n",
        "\n",
        "Note that our `update` function calls our `loss` function -- so although we didn't decorate our `loss` function with `@jax.jit` directly we'll still reap the benefits when we call `update`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0OrGss88syY"
      },
      "source": [
        "def loss(params, rng, batch_token_ids, batch_labels):\n",
        "    logits = roberta_classifier.apply(params, rng, batch_token_ids, training=True)\n",
        "    labels = hk.one_hot(batch_labels, config['n_classes'])\n",
        "    softmax_xent = -jnp.sum(labels * jax.nn.log_softmax(logits))\n",
        "    softmax_xent /= labels.shape[0]\n",
        "    return softmax_xent\n",
        "\n",
        "@jax.jit\n",
        "def accuracy(params, rng, batch_token_ids, batch_labels):\n",
        "    predictions = roberta_classifier.apply(params, rng, batch_token_ids, training=False)\n",
        "    return jnp.mean(jnp.argmax(predictions, axis=-1) == batch_labels)\n",
        "\n",
        "@jax.jit\n",
        "def update(params, rng, opt_state, batch_token_ids, batch_labels):\n",
        "    batch_loss, grads = jax.value_and_grad(loss)(params, rng, batch_token_ids, batch_labels)\n",
        "    updates, opt_state = opt.update(grads, opt_state)\n",
        "    new_params = optix.apply_updates(params, updates)\n",
        "    return new_params, opt_state, batch_loss\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9RzrI8e8syc"
      },
      "source": [
        "Finetuning transformers for downstream tasks requires a few tricks for reliable performance -- we'll use a linear warmup + decay along with gradient clipping for our optimizers. JAX exposes 2 different sets of optimization utilities in `jax.experimental.optimizers` and `jax.experimental.optix` respectively. For finetuning RoBERTa we'll be using the latter, as it includes learning rate schedule utilities through `optix.scale_by_schedule` as well as a utility for gradient clipping with `optix.clip_by_global_norm`. We can apply our bag of optimization tricks in combination with a vanilla adam optimizer using `optix.chain` as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-uWRAsMIS4L"
      },
      "source": [
        "def make_lr_schedule(warmup_percentage, total_steps):\n",
        "    def lr_schedule(step):\n",
        "        percent_complete = step / total_steps\n",
        "        before_peak = jax.lax.convert_element_type(\n",
        "            (percent_complete <= warmup_percentage),\n",
        "            np.float32\n",
        "        )\n",
        "        scale = (\n",
        "            (before_peak * (percent_complete / warmup_percentage) + (1 - before_peak))\n",
        "            * (1 - percent_complete)\n",
        "        )\n",
        "        return scale\n",
        "    return lr_schedule\n",
        "\n",
        "\n",
        "total_steps = config['n_epochs'] * (n_examples // config['batch_size'])\n",
        "lr_schedule = make_lr_schedule(warmup_percentage=0.1, total_steps=total_steps)\n",
        "opt = optix.chain(\n",
        "    optix.clip_by_global_norm(config['max_grad_norm']),\n",
        "    optix.adam(learning_rate=config['learning_rate']),\n",
        "    optix.scale_by_schedule(lr_schedule)\n",
        ")\n",
        "opt_state = opt.init(params)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRcUMlnr8syf"
      },
      "source": [
        "Below, we throw together one final utility before writing our training loop -- a short convenience function to print how our train and test accuracy change over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rglzrhxC8syg"
      },
      "source": [
        "def measure_current_performance(params, n_examples=None, splits=('train', 'test')):\n",
        "    # Load our training evaluation and test evaluation splits\n",
        "    if 'train' in splits:\n",
        "        train_eval = load_dataset(\"train\", training=False, batch_size=25, n_examples=n_examples)\n",
        "        # Compute mean train accuracy\n",
        "        train_accuracy = np.mean([\n",
        "            accuracy(\n",
        "                params,\n",
        "                rng,\n",
        "                encode_batch(train_eval_batch['text']),\n",
        "                train_eval_batch['label']\n",
        "            )\n",
        "            for train_eval_batch in train_eval\n",
        "        ])\n",
        "        print(f\"\\t Train acc: {train_accuracy:.3f}\")\n",
        "\n",
        "    if 'test' in splits:\n",
        "        test_eval = load_dataset(\"test\", training=False, batch_size=25, n_examples=n_examples)\n",
        "        # Compute mean test accuracy\n",
        "        test_accuracy = np.mean([\n",
        "            accuracy(\n",
        "                params,\n",
        "                rng,\n",
        "                encode_batch(test_eval_batch['text']),\n",
        "                test_eval_batch['label'],\n",
        "            )\n",
        "            for test_eval_batch in test_eval\n",
        "        ])\n",
        "        print(f\"\\t Test accuracy: {test_accuracy:.3f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFqwpsBB8syj"
      },
      "source": [
        "In our training loop, we simply pull batches of examples from our training data iterator and call the update function to modify the state of our parameters.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8QKBe7J8syj"
      },
      "source": [
        "for step, train_batch in enumerate(train):\n",
        "    if step % 100 == 0:\n",
        "        print(f\"[Step {step}]\")\n",
        "    if step % 1000 == 0 and step != 0:\n",
        "        measure_current_performance(params, n_examples=100)\n",
        "\n",
        "    # Perform adam update\n",
        "    next_batch = next(train)\n",
        "    batch_token_ids = encode_batch(next_batch['text'])\n",
        "    batch_labels = next_batch['label']\n",
        "    params, opt_state, batch_loss = update(\n",
        "        params, rng, opt_state, batch_token_ids, batch_labels\n",
        "    )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdKkud6u8syn"
      },
      "source": [
        "When all is said and done, we achieve a respectable test accuracy of 0.xx on the IMDB review dataset. Not too shabby!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLHm04RcIyIB"
      },
      "source": [
        "measure_current_performance(params, n_examples=25000, splits='test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY9PEiep8syq"
      },
      "source": []
    }
  ]
}